---
layout: project
title: "A multi-modal robotic control system that interprets high-level natural language commands"
year: "2025"
tech_stack: "Python, LLM, Vision"
status: "Ongoing"
permalink: /projects/project1_LLM_controlled_robot/
links:
  - name: "GitHub"
    url: "https://github.com/jaehyeokoh/human-robot-communication"
#   - name: "Paper"
#     url: "#"
#   - name: "Demo"
#     url: "#"
---

## Abstract

Recently, the performance of large language models (LLMs) has been continuously improving. There have been various attempts to apply LLMs to robotics, and foundation models trained entirely for robots are emerging. However, creating training data and training models for robots is difficult for ordinary users to access. Therefore, I introduce a project that controls a robot using off-the-shelf LLM (that a general-purpose LLM rather than a robot-specialized one : GPT). In pipeline, the LLM serves as a high-level planner that performs planning by leveraging its greatest strength, natural-language generation, while delegating the LLMâ€™s weakness(e.g spatial coordinate calculation) to other external modules such as DINO , SAM, Anygrasp. I also present a method that mutually compensates for two drawbacks: the weakness of VLMs in outputting concrete coordinates, and the DINO modelâ€™s inability to provide natural-language state descriptions about the detected object. As a result, this project shows that the system can execute tasks ranging from simple pick-and-place commands (â€œgrasp the ~â€), to interpreting user instructions (â€œgive me something to eatâ€, â€œgrab what looks like it may fall from the deskâ€), and further, high level commands that require multi-step reasoning (â€œmake a dolmen shape using what is visible on the screenâ€).

## Overview of system

{% include project-media.html
   type="image"
   src="full_map.jpg"
   caption="Fig1"
   size="full"
%}


## Example video (2x Speed)

<div class="media-grid-2">
  {% include project-media.html type="video" src="neuro1.mp4" caption="input : í° ë°•ìŠ¤ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë¬¼ê±´ë“¤ì„ ì‘ì€ ê²ƒë¶€í„° ìˆœì„œëŒ€ë¡œ í•˜ë‚˜ëŠ” í° ë°•ìŠ¤ ì•ì—, í•˜ë‚˜ëŠ” í° ë°•ìŠ¤ ìœ„ì—, í•˜ë‚˜ëŠ” í° ë°•ìŠ¤ ë’¤ìª½ì— ë†”ì¤˜" muted=true autoplay=true loop=true%}
  {% include project-media.html type="video" src="neuro_beverage.mp4" caption="input : ë¨¹ì„ ìˆ˜ ìˆëŠ” ê±´ í° ë°•ìŠ¤ ì•ì—, ë¨¹ì„ ìˆ˜ ì—†ëŠ” ê±´ í° ë°•ìŠ¤ ë’¤ì— ë†”" muted=true autoplay=true loop=true%}
</div>

<div class="media-grid-2">
  {% include project-media.html type="video" src="falling_obj_catch.mp4" caption="input : ê³§ ë–¨ì–´ì§ˆ ê²ƒ ê°™ì€ ë¬¼ê±´ì„ ì¡ì•„" muted=true autoplay=true loop=true%}
  {% include project-media.html type="video" src="food_name.mp4" caption="input : í¬ì¹´ë¦¬ ìŠ¤ì›¨íŠ¸ë¥¼ í›„ë ˆì‰¬ ë² ë¦¬ ìœ„ì— ì˜¬ë ¤" muted=true autoplay=true loop=true%}
</div>

<div class="media-grid-2">
  {% include project-media.html type="video" src="proper_crate.mp4" caption="input : ìŒë£Œìˆ˜ë¥¼ ì ì ˆí•œ ìƒìì— ë†”" muted=true autoplay=true loop=true%}
  {% include project-media.html type="video" src="food_crate.mp4" caption="input : ìŒë£Œìˆ˜ë¥¼ ìŒì‹ ìƒìì— ë†”" muted=true autoplay=true loop=true%}
</div>

{% include project-media.html
   type="video"
   src="complex_input.mp4"
   caption="input : ë‚˜ ëª©ë§ˆë¥´ë‹ˆ ì ì ˆí•œ ë¬¼ê±´ì„ í° ìƒì ìœ„ì— ì˜¬ë ¤ ê·¸ë¦¬ê³  ë‚˜ë¨¸ì§€ ë¬¼ê±´ë“¤ì„ í°ê²ƒ ë¶€í„° í•˜ë‚˜ëŠ” í° ìƒì ì˜¤ë¥¸ìª½ì— í•˜ë‚˜ëŠ” í° ìƒì ì•ì— ë†”"
   autoplay=true
   muted=true
   loop=true
   size="medium"
%}

## Introduction

Recent advances in large language models (LLMs) and vision-language models (VLMs) have expanded the potential for natural-language-driven robotic control. Research prototypes such as RT-2 have shown that foundation models can directly interpret visual scenes and map them to robotic actions. However, these systems typically require extensive domain-specific training data and computational resources, which are inaccessible to most users. Furthermore, while VLMs can recognize and describe visual content, they often lack the ability to output precise spatial coordinates, and object detection models like DINO cannot provide rich natural-language descriptions of their detections.

This project addresses these gaps by demonstrating a fully functional robot control pipeline powered by an off-the-shelf, general-purpose LLM (GPT) without any robot-specific fine-tuning. The LLM is used solely as a high-level planner, delegating tasks such as spatial reasoning, segmentation, and grasp point generation to specialized external modules (Grounding DINO, SAM, AnyGrasp). A novel label-visualization and VLM-selection method bridges the gap between VLMs and object detectors, enabling robust object targeting without relying on manual text prompts. In addition, point-cloud interpolation and SAM-call minimization techniques overcome occlusion and performance bottlenecks, allowing real-time execution even on mid-range hardware.

By combining these techniques, the system executes a wide range of commands â€” from simple pick-and-place tasks to multi-step reasoning challenges â€” illustrating a practical, accessible approach to robot control that leverages the strengths of existing foundation models while mitigating their weaknesses.

## Methods

### 1. Perception


{% include project-media.html
   type="image"
   src="perception_description.png"
   caption="Fig2"
   size="full"
%}

{% include project-media.html
   type="image"
   src="perception_map.png"
   caption="Fig3"
   size="full"
%}

## Research Objectives

- **Natural Language Processing**: Implement advanced NLP algorithms to understand human speech and intent
- **Computer Vision**: Develop visual recognition systems for gesture and facial expression analysis
- **Multimodal Integration**: Combine audio, visual, and contextual information for better understanding
- **Real-time Response**: Ensure low-latency communication for natural interaction flow

## Technical Approach

### 1. Speech Recognition Module

- Custom speech recognition model trained on conversational data
- Real-time audio processing using Python and ROS
- Integration with existing robot control systems

{% include project-media.html
   type="gif"
   src="speech-demo.gif"
   caption="Real-time speech recognition demonstration"
   size="medium"
%}

### 2. Visual Communication Analysis

- OpenCV-based facial expression recognition
- Hand gesture detection and classification
- Body language interpretation algorithms

{% include project-media.html
   type="video"
   src="neuro1.mp4"
   caption="Hand gesture recognition system in action"
   autoplay=true
   muted=true
   loop=true
%}

### 3. Response Generation

- Context-aware response selection
- Natural language generation for robot speech
- Coordinated verbal and physical responses

## Demo Video

{% include project-media.html
   type="youtube"
   id="dQw4w9WgXcQ"
   caption="Full system demonstration (YouTube)"
%}

## Current Progress

- âœ… Basic speech recognition implementation
- âœ… Facial expression detection prototype
- ğŸ”„ Multimodal fusion algorithm development
- â³ Real-time testing on robot platform
- â³ User study preparation

## Challenges & Solutions

**Challenge**: Real-time processing requirements for natural interaction  
**Solution**: Optimized algorithms and parallel processing implementation

**Challenge**: Handling noisy environments and varying speech patterns  
**Solution**: Robust preprocessing and adaptive learning mechanisms

## Future Work

- Integration with physical robot platform
- Extensive user testing and feedback collection
- Publication of research findings
- Open-source release of core modules

## Related Publications

*Research paper in preparation for submission to ICRA 2025*